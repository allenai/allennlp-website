---
---
<!DOCTYPE html>
<html lang="en-us">
  <head>
    {% include meta.html %}
    <title>AllenNLP - Publications</title>
  </head>
  <body data-page="publications">
    <div id="page-content">
      {% include header.html %}
      <div class="banner banner--interior-hero">
        <div class="constrained constrained--sm">
          <div class="banner--interior-hero__content">
            <h1>Publications</h1>
            <p class="t-sm">A core goal of the AllenNLP team is to push the field of natural language processing forward through cutting-edge research. See the publications from our team.</p>
          </div>
        </div>
      </div>
      <div class="constrained constrained--med">
        <ul class="publications">
          <li><a href="http://www.allennlp.org/papers/AllenNLP_white_paper.pdf" target="_blank" class="publications__title">AllenNLP: A Deep Semantic Natural Language Processing Platform</a><span class="publications__authors">Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, Luke Zettlemoyer</span><span class="publications__year">2017</span>
            <p class="publications__abstract">This paper describes AllenNLP, a platform for research on deep learning methods in natural language understanding. AllenNLP is designed to support researchers who want to build novel language understanding models quickly and easily. It is built on top of PyTorch, allowing for dynamic computation graphs, and provides (1) a flexible data API that handles intelligent batching and padding, (2) highlevel abstractions for common operations...</p>
          </li>
          <li><a href="https://homes.cs.washington.edu/~luheng/files/acl2017_hllz.pdf" target="_blank" class="publications__title">Deep Semantic Role Labeling: What Works and What’s Next</a><span class="publications__authors">Luheng He, Kenton Lee, Mike Lewis, Luke Zettlemoyer</span><span class="publications__year">2017</span>
            <p class="publications__abstract">We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on the CoNLL 2005 test set and 83.4 F1 on...</p>
          </li>
          <li><a href="https://www.semanticscholar.org/paper/Neural-Semantic-Parsing-with-Type-Constraints-for-Krishnamurthy-Dasigi/ad1eff3351005377499835e1e29b901b1a75467f" target="_blank" class="publications__title">Neural Semantic Parsing with Type Constraints for Semi-Structured Tables</a><span class="publications__authors">Jayant Krishnamurthy, Pradeep Dasigi, Matt Gardner</span><span class="publications__year">2017</span>
            <p class="publications__abstract">We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations: (1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for...</p>
          </li>
          <li><a href="https://www.semanticscholar.org/paper/End-to-end-Neural-Coreference-Resolution-Lee-He/c30fac16cdf5aa52ffd78c242515a572d7809ea8" target="_blank" class="publications__title">End-to-end Neural Coreference Resolution</a><span class="publications__authors">Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer</span><span class="publications__year">2017</span>
            <p class="publications__abstract">We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with...</p>
          </li>
          <li><a href="https://www.semanticscholar.org/paper/The-AI2-system-at-SemEval-2017-Task-10-ScienceIE-s-Ammar-Peters/2264e14e35dc5a3db93437bc408a03171af8c59d" target="_blank" class="publications__title">The AI2 system at SemEval-2017 Task 10 (ScienceIE): semi-supervised end-to-end entity and relation extraction</a><span class="publications__authors">Waleed Ammar, Matthew E. Peters, Chandra Bhagavatula, Russell Power</span><span class="publications__year">2017</span>
            <p class="publications__abstract">This paper describes our submission for the ScienceIE shared task (SemEval2017 Task 10) on entity and relation extraction from scientific papers. Our model is based on the end-to-end relation extraction model of Miwa and Bansal (2016) with several enhancements such as semi-supervised learning via neural language models, character-level encoding, gazetteers extracted from existing knowledge bases, and model ensembles. Our...</p>
          </li>
          <li><a href="https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectiona-Peters-Ammar/821121c81c2b1da65ac546d83ed0149829456d08" target="_blank" class="publications__title">Semi-supervised sequence tagging with bidirectional language models</a><span class="publications__authors">Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power</span><span class="publications__year">2017</span>
            <p class="publications__abstract">Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network archi-tectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre-trained context embeddings...</p>
          </li>
          <li><a href="https://www.semanticscholar.org/paper/Open-Vocabulary-Semantic-Parsing-with-both-Distrib-Gardner-Krishnamurthy/6fc677b7f3319b344eddd93e93ca4d65005f6cd2" target="_blank" class="publications__title">Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge</a><span class="publications__authors">Matt Gardner, Jayant Krishnamurthy</span><span class="publications__year">2017</span>
            <p class="publications__abstract">Traditional semantic parsers map language onto composi-tional, executable queries in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (KBs, e.g., Freebase) to answer questions, but it is also fundamentally limiting— these semantic parsers can only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed...</p>
          </li>
          <li><a href="https://www.semanticscholar.org/paper/Commonly-Uncommon-Semantic-Sparsity-in-Situation-R-Yatskar-Ordonez/02239ae5e922075a354169f75f684cad8fdfd5ab" target="_blank" class="publications__title">Commonly Uncommon: Semantic Sparsity in Situation Recognition</a><span class="publications__authors">Mark Yatskar, Vicente Ordonez, Luke S. Zettlemoyer, Ali Farhadi</span><span class="publications__year">2016</span>
            <p class="publications__abstract">Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images , including activities, objects and the roles objects play within the activity. For this...</p>
          </li>
        </ul>
      </div>
      {% include footer.html %}
    </div>
    {% include svg-sprite.html %}
    {% include scripts.html %}
  </body>
</html>
