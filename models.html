---
---
<!DOCTYPE html>
<html lang="en-us">
  <head>
    {% include meta.html %}
    <title>AllenNLP - Models</title>
  </head>

  <body data-page="models">
    <div id="page-content">
      {% include header.html %}
      <div class="banner banner--interior-hero">
        <div class="constrained constrained--sm">
          <div class="banner--interior-hero__content">
            <h1>Models</h1>
            <p class="t-sm">See how our models compare to the competition. AllenNLP provides strong performance with reasonable runtimes, along with the infrastructure to easily run them.</p>
          </div>
        </div>
      </div>
      <div class="banner c-bg-gray-light">
        <div class="constrained constrained--med">
          <div class="padded-med">
            <h3>A Note on Non-determinism</h3>
            <p class="t-sm">It is well known (see <a href="https://www.semanticscholar.org/paper/Reporting-Score-Distributions-Makes-a-Difference-P-Reimers-Gurevych/0eae432f7edacb262f3434ecdb2af707b5b06481">Reimers and Gurevych, 2017</a>) that multiple runs of probabilistic deep learning algorithms can have large variance in overall scores.  Ideally this variance could be controlled by setting random-number generator seeds, and others would be able to retrain models with reproducible results.  Unfortunately, some CuDNN methods are non-deterministic (see section 2.5 of the <a href="https://docs.nvidia.com/deeplearning/sdk/pdf/cuDNN-Developer-Guide.pdf">CuDNN Developer's Guide</a>) and the rest are only deterministic on the same architecture, with the same number of GPU multiprocessors, and using the same version of CuDNN.</p>
            <p class="t-sm">We provide the best models we trained.  In other words, we have trained our models multiple times and selected the best result.  Please keep in mind that if you retrain any of the following models, you are likely to end up with a slightly worse evaluation.</p>
          </div>
        </div>
      </div>
      <div class="banner">
        <div class="constrained constrained--med">
          <div class="padded-med">
            <h2>Machine Comprehension</h2>
            <p class="t-sm">Machine Comprehension (MC) models answer natural language questions by selecting an answer span within an evidence text. The AllenNLP MC model is a reimplementation of <a href="https://www.semanticscholar.org/paper/Bidirectional-Attention-Flow-for-Machine-Comprehen-Seo-Kembhavi/007ab5528b3bd310a80d553cccad4b78dc496b02" target="_blank">BiDAF (Seo et al, 2017)</a>, or Bi-Directional Attention Flow, a widely used MC baseline that achieves near state-of-the-art accuracies on <a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank">the SQuAD dataset</a>.  The AllenNLP BIDAF model achieves an EM score of 68.3 on the SQuAD dev set, just slightly ahead of the original BIDAF system's score of 67.7, while also training at a 10x speedup (4 hours on a p2.xlarge).</p>
          </div>
        </div>
      </div>
      <div class="banner c-bg-gray-light">
        <div class="constrained constrained--med">
          <div class="padded-med">
            <h2>Semantic Role Labeling</h2>
            <p class="t-sm">SRL, or Semantic Role Labeling, models recover the latent predicate argument structure of a sentence. SRL builds representations that answer basic questions about sentence meaning, including "who" did "what" to â€œwhom," etc. The AllenNLP SRL model is a reimplementation of <a href="https://www.semanticscholar.org/paper/Deep-Semantic-Role-Labeling-What-Works-and-What-s-He-Lee/a3ccff7ad63c2805078b34b8514fa9eab80d38e9" target="_blank">a deep BiLSTM model (He et al, 2017)</a>. The AllenNLP SRL model closely matches the published model, achieving a F1 of 78.9 on <a href="http://cemantix.org/data/ontonotes.html" target="_blank">English Ontonotes 5.0 dataset using the CONLL 2011/12 shared task format</a>.</p>
          </div>
        </div>
      </div>
      <div class="banner">
        <div class="constrained constrained--med">
          <div class="padded-med">
            <h2>Textual Entailment</h2>
            <p class="t-sm">Textual Entailment (TE) models take a pair of sentences and predict whether the facts in the first necessarily imply the facts in the second one. The AllenNLP TE model is a reimplementation of <a href="https://www.semanticscholar.org/paper/A-Decomposable-Attention-Model-for-Natural-Languag-Parikh-T%C3%A4ckstr%C3%B6m/07a9478e87a8304fc3267fa16e83e9f3bbd98b27" target="_blank">the decomposable attention model (Parikh et al, 2017)</a>, a widely used TE baseline that is relatively simple and achieves near state-of-the-art performance on<a href="https://nlp.stanford.edu/projects/snli/" target="_blank">the SNLI dataset</a>.  The AllenNLP TE model achieves an accuracy of 84.7% on the SNLI 1.0 test dataset, which is comparable to the original system's score of 86.3%.</p>
          </div>
        </div>
      </div>
      {% include footer.html %}
    </div>
    {% include svg-sprite.html %}
    {% include scripts.html %}
  </body>
</html>
